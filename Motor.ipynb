{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Motor.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjzkmNnnYs2E",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/SherazKhan/Motor/blob/master/human-activity.jpg?raw=1'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6vPTuArYs2E",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<center>    \n",
        "    <h1> Human Activity Recognition </h1>    \n",
        "</center>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFZiEVp2Ys2F",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "https://medium.com/@rubeen.786.mr/human-activity-recognition-har-db5c1432cd98\n",
        "\n",
        "The main idea for this notebook is to build a model that predicts the human activities such as Walking, Walking_Upstairs, Walking_Downstairs, Sitting, Standing or Laying.\n",
        "\n",
        "This dataset is collected from 30 persons(referred as subjects in this dataset), performing different activities with a smartphone to their waists. The data is recorded with the help of sensors (accelerometer and Gyroscope) in that smartphone. This experiment was video recorded to label the data manually.\n",
        "\n",
        "This dataset is downloaded from:\n",
        "\n",
        "http://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones\n",
        "\n",
        "## How data was recorded\n",
        "\n",
        "By using the sensors(Gyroscope and accelerometer) in a smartphone, they have captured '3-axial linear acceleration'(_tAcc-XYZ_) from accelerometer and '3-axial angular velocity' (_tGyro-XYZ_) from Gyroscope with several variations. \n",
        "\n",
        "> prefix 't' in those metrics denotes time.\n",
        "\n",
        "> suffix 'XYZ' represents 3-axial signals in X , Y, and Z directions.\n",
        "\n",
        "### Feature names\n",
        "\n",
        "1. These sensor signals are preprocessed by applying noise filters and then sampled in fixed-width windows(sliding windows) of 2.56 seconds each with 50% overlap. ie., each window has 128 readings. \n",
        "\n",
        "2. From Each window, a feature vector was obtianed by calculating variables from the time and frequency domain.\n",
        "> In our dataset, each datapoint represents a window with different readings \n",
        "3. The accelertion signal was saperated into Body and Gravity acceleration signals(___tBodyAcc-XYZ___ and ___tGravityAcc-XYZ___) using some low pass filter with corner frequecy of 0.3Hz.\n",
        "\n",
        "4. After that, the body linear acceleration and angular velocity were derived in time to obtian _jerk signals_ (___tBodyAccJerk-XYZ___ and ___tBodyGyroJerk-XYZ___). \n",
        "\n",
        "5. The magnitude of these 3-dimensional signals were calculated using the Euclidian norm. This magnitudes are represented as features with names like _tBodyAccMag_, _tGravityAccMag_, _tBodyAccJerkMag_, _tBodyGyroMag_ and _tBodyGyroJerkMag_.\n",
        "\n",
        "6. Finally, We've got frequency domain signals from some of the available signals by applying a FFT (Fast Fourier Transform). These signals obtained were labeled with ___prefix 'f'___ just like original signals with ___prefix 't'___. These signals are labeled as ___fBodyAcc-XYZ___, ___fBodyGyroMag___ etc.,.\n",
        "\n",
        "7. These are the signals that we got so far.\n",
        "\t+ tBodyAcc-XYZ\n",
        "\t+ tGravityAcc-XYZ\n",
        "\t+ tBodyAccJerk-XYZ\n",
        "\t+ tBodyGyro-XYZ\n",
        "\t+ tBodyGyroJerk-XYZ\n",
        "\t+ tBodyAccMag\n",
        "\t+ tGravityAccMag\n",
        "\t+ tBodyAccJerkMag\n",
        "\t+ tBodyGyroMag\n",
        "\t+ tBodyGyroJerkMag\n",
        "\t+ fBodyAcc-XYZ\n",
        "\t+ fBodyAccJerk-XYZ\n",
        "\t+ fBodyGyro-XYZ\n",
        "\t+ fBodyAccMag\n",
        "\t+ fBodyAccJerkMag\n",
        "\t+ fBodyGyroMag\n",
        "\t+ fBodyGyroJerkMag\n",
        "\n",
        "8. We can esitmate some set of variables from the above signals. ie., We will estimate the following properties on each and every signal that we recoreded so far.\n",
        "\n",
        "\t+ ___mean()___: Mean value\n",
        "\t+ ___std()___: Standard deviation\n",
        "\t+ ___mad()___: Median absolute deviation \n",
        "\t+ ___max()___: Largest value in array\n",
        "\t+ ___min()___: Smallest value in array\n",
        "\t+ ___sma()___: Signal magnitude area\n",
        "\t+ ___energy()___: Energy measure. Sum of the squares divided by the number of values. \n",
        "\t+ ___iqr()___: Interquartile range \n",
        "\t+ ___entropy()___: Signal entropy\n",
        "\t+ ___arCoeff()___: Autorregresion coefficients with Burg order equal to 4\n",
        "\t+ ___correlation()___: correlation coefficient between two signals\n",
        "\t+ ___maxInds()___: index of the frequency component with largest magnitude\n",
        "\t+ ___meanFreq()___: Weighted average of the frequency components to obtain a mean frequency\n",
        "\t+ ___skewness()___: skewness of the frequency domain signal \n",
        "\t+ ___kurtosis()___: kurtosis of the frequency domain signal \n",
        "\t+ ___bandsEnergy()___: Energy of a frequency interval within the 64 bins of the FFT of each window.\n",
        "\t+ ___angle()___: Angle between to vectors.\n",
        "\n",
        "9. We can obtain some other vectors by taking the average of signals in a single window sample. These are used on the angle() variable'\n",
        "`\n",
        "\t+ gravityMean\n",
        "\t+ tBodyAccMean\n",
        "\t+ tBodyAccJerkMean\n",
        "\t+ tBodyGyroMean\n",
        "\t+ tBodyGyroJerkMean\n",
        "\n",
        "\n",
        "###  Y_Labels(Encoded)\n",
        "+ In the dataset, Y_labels are represented as numbers from 1 to 6 as their identifiers.\n",
        "\n",
        "\t- WALKING as __1__\n",
        "\t- WALKING_UPSTAIRS as __2__\n",
        "\t- WALKING_DOWNSTAIRS as __3__\n",
        "\t- SITTING as __4__\n",
        "\t- STANDING as __5__\n",
        "\t- LAYING as __6__\n",
        "    \n",
        "## Train and test data were saperated\n",
        " - The readings from ___70%___ of the volunteers were taken as ___trianing data___ and remaining ___30%___ subjects recordings were taken for ___test data___\n",
        " \n",
        "## Data\n",
        "\n",
        "* All the data is present in 'UCI_HAR_dataset/' folder in present working directory.\n",
        "     - Feature names are present in 'UCI_HAR_dataset/features.txt'\n",
        "     - ___Train Data___\n",
        "         - 'UCI_HAR_dataset/train/X_train.txt'\n",
        "         - 'UCI_HAR_dataset/train/subject_train.txt'\n",
        "         - 'UCI_HAR_dataset/train/y_train.txt'\n",
        "     - ___Test Data___\n",
        "         - 'UCI_HAR_dataset/test/X_test.txt'\n",
        "         - 'UCI_HAR_dataset/test/subject_test.txt'\n",
        "         - 'UCI_HAR_dataset/test/y_test.txt'\n",
        "         \n",
        "\n",
        "## Data Size :\n",
        "> 282 MB\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5-FYy9OboBa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "76d10e5c-d784-42f8-dbe3-c6eca9fea118"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "!pip install pandas==0.20.0\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pandas==0.20.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/1a/11c05627ba733c40ab38ec7c693bb0a145ad5ad88bcfc8c9e99764cc4c2b/pandas-0.20.0.tar.gz (10.3MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3MB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas==0.20.0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas==0.20.0) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.20.0) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas==0.20.0) (1.15.0)\n",
            "Building wheels for collected packages: pandas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqATFbCgYs2J",
        "colab_type": "text"
      },
      "source": [
        "# Quick overview of the dataset :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQWctqUOYs2J",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "* Accelerometer and Gyroscope readings are taken from 30 volunteers(referred as subjects) while performing the following 6 Activities.\n",
        "\n",
        "    1. Walking     \n",
        "    2. WalkingUpstairs \n",
        "    3. WalkingDownstairs \n",
        "    4. Standing \n",
        "    5. Sitting \n",
        "    6. Lying.\n",
        "\n",
        "\n",
        "* Readings are divided into a window of 2.56 seconds with 50% overlapping. \n",
        "\n",
        "* Accelerometer readings are divided into gravity acceleration and body acceleration readings,\n",
        "  which has x,y and z components each.\n",
        "\n",
        "* Gyroscope readings are the measure of angular velocities which has x,y and z components.\n",
        "\n",
        "* Jerk signals are calculated for BodyAcceleration readings.\n",
        "\n",
        "* Fourier Transforms are made on the above time readings to obtain frequency readings.\n",
        "\n",
        "* Now, on all the base signal readings., mean, max, mad, sma, arcoefficient, engerybands,entropy etc., are calculated for each window.\n",
        "\n",
        "* We get a feature vector of 561 features and these features are given in the dataset.\n",
        "\n",
        "* Each window of readings is a datapoint of 561 features.\n",
        "\n",
        "## Problem Framework\n",
        "\n",
        "* 30 subjects(volunteers) data is randomly split to 70%(21) test and 30%(7) train data.\n",
        "* Each datapoint corresponds one of the 6 Activities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ8wz0EYYs2J",
        "colab_type": "text"
      },
      "source": [
        "## Problem Statement\n",
        "\n",
        " + Given a new datapoint we have to predict the Activity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAjOEFGjYs2K",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj9vF5YyYs2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"font.family\"] = 'DejaVu Sans'\n",
        "import seaborn as sns\n",
        "\n",
        "# To be able to save images on server\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Importing tensorflow\n",
        "np.random.seed(42)\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM , BatchNormalization\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras.regularizers import L1L2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpT-CkCFYs2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ad33412-fd45-4e68-b39b-0d43fcf34ade"
      },
      "source": [
        "# get the features from the file features.txt\n",
        "features = list()\n",
        "with open('https://github.com/SherazKhan/Motor/tree/master/UCI_HAR_dataset/features.txt') as f:\n",
        "    features = [line.split()[1] for line in f.readlines()]\n",
        "print('No of Features: {}'.format(len(features)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of Features: 561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSLzv6ivYs2P",
        "colab_type": "text"
      },
      "source": [
        "## Obtain the  train data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB_ZhB9AYs2Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "a57043c8-610e-48f5-a811-e930d5859f97"
      },
      "source": [
        "# get the data from txt files to pandas dataffame\n",
        "X_train = pd.read_csv('https://github.com/SherazKhan/Motor/tree/master/UCI_HAR_dataset/train/X_train.txt', delim_whitespace=True, header=None, names=features)\n",
        "\n",
        "# add subject column to the dataframe\n",
        "X_train['subject'] = pd.read_csv('https://github.com/SherazKhan/Motor/tree/master/UCI_HAR_dataset/train/subject_train.txt', header=None, squeeze=True)\n",
        "\n",
        "y_train = pd.read_csv('https://github.com/SherazKhan/Motor/tree/master/UCI_HAR_dataset/train/y_train.txt', names=['Activity'], squeeze=True)\n",
        "y_train_labels = y_train.map({1: 'WALKING', 2:'WALKING_UPSTAIRS',3:'WALKING_DOWNSTAIRS',\\\n",
        "                       4:'SITTING', 5:'STANDING',6:'LAYING'})\n",
        "\n",
        "# put all columns in a single dataframe\n",
        "train = X_train\n",
        "train['Activity'] = y_train\n",
        "train['ActivityName'] = y_train_labels\n",
        "train.sample()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style>\n",
              "    .dataframe thead tr:only-child th {\n",
              "        text-align: right;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tBodyAcc-mean()-X</th>\n",
              "      <th>tBodyAcc-mean()-Y</th>\n",
              "      <th>tBodyAcc-mean()-Z</th>\n",
              "      <th>tBodyAcc-std()-X</th>\n",
              "      <th>tBodyAcc-std()-Y</th>\n",
              "      <th>tBodyAcc-std()-Z</th>\n",
              "      <th>tBodyAcc-mad()-X</th>\n",
              "      <th>tBodyAcc-mad()-Y</th>\n",
              "      <th>tBodyAcc-mad()-Z</th>\n",
              "      <th>tBodyAcc-max()-X</th>\n",
              "      <th>...</th>\n",
              "      <th>angle(tBodyAccMean,gravity)</th>\n",
              "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
              "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
              "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
              "      <th>angle(X,gravityMean)</th>\n",
              "      <th>angle(Y,gravityMean)</th>\n",
              "      <th>angle(Z,gravityMean)</th>\n",
              "      <th>subject</th>\n",
              "      <th>Activity</th>\n",
              "      <th>ActivityName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4525</th>\n",
              "      <td>0.283203</td>\n",
              "      <td>-0.047024</td>\n",
              "      <td>-0.168986</td>\n",
              "      <td>0.384949</td>\n",
              "      <td>0.176898</td>\n",
              "      <td>-0.310332</td>\n",
              "      <td>0.381757</td>\n",
              "      <td>0.122611</td>\n",
              "      <td>-0.332984</td>\n",
              "      <td>0.465563</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.034924</td>\n",
              "      <td>0.558036</td>\n",
              "      <td>0.258975</td>\n",
              "      <td>-0.854858</td>\n",
              "      <td>-0.78433</td>\n",
              "      <td>0.22296</td>\n",
              "      <td>-0.066506</td>\n",
              "      <td>22</td>\n",
              "      <td>3</td>\n",
              "      <td>WALKING_DOWNSTAIRS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 564 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  \\\n",
              "4525           0.283203          -0.047024          -0.168986   \n",
              "\n",
              "      tBodyAcc-std()-X  tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  \\\n",
              "4525          0.384949          0.176898         -0.310332          0.381757   \n",
              "\n",
              "      tBodyAcc-mad()-Y  tBodyAcc-mad()-Z  tBodyAcc-max()-X  \\\n",
              "4525          0.122611         -0.332984          0.465563   \n",
              "\n",
              "             ...          angle(tBodyAccMean,gravity)  \\\n",
              "4525         ...                            -0.034924   \n",
              "\n",
              "      angle(tBodyAccJerkMean),gravityMean)  angle(tBodyGyroMean,gravityMean)  \\\n",
              "4525                              0.558036                          0.258975   \n",
              "\n",
              "      angle(tBodyGyroJerkMean,gravityMean)  angle(X,gravityMean)  \\\n",
              "4525                             -0.854858              -0.78433   \n",
              "\n",
              "      angle(Y,gravityMean)  angle(Z,gravityMean)  subject  Activity  \\\n",
              "4525               0.22296             -0.066506       22         3   \n",
              "\n",
              "            ActivityName  \n",
              "4525  WALKING_DOWNSTAIRS  \n",
              "\n",
              "[1 rows x 564 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "H1CEv1X0Ys2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo8yNH5vYs2U",
        "colab_type": "text"
      },
      "source": [
        "## Obtain the  test data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ9MbAqKYs2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the data from txt files to pandas dataffame\n",
        "X_test = pd.read_csv('/content/drive/My Drive/Motor/UCI_HAR_dataset/test/X_test.txt', delim_whitespace=True, header=None, names=features)\n",
        "\n",
        "# add subject column to the dataframe\n",
        "X_test['subject'] = pd.read_csv('/content/drive/My Drive/Motor/UCI_HAR_dataset/test/subject_test.txt', header=None, squeeze=True)\n",
        "\n",
        "# get y labels from the txt file\n",
        "y_test = pd.read_csv('/content/drive/My Drive/Motor/UCI_HAR_dataset/test/y_test.txt', names=['Activity'], squeeze=True)\n",
        "y_test_labels = y_test.map({1: 'WALKING', 2:'WALKING_UPSTAIRS',3:'WALKING_DOWNSTAIRS',\\\n",
        "                       4:'SITTING', 5:'STANDING',6:'LAYING'})\n",
        "\n",
        "\n",
        "# put all columns in a single dataframe\n",
        "test = X_test\n",
        "test['Activity'] = y_test\n",
        "test['ActivityName'] = y_test_labels\n",
        "test.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KotUhX85Ys2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpX699fqYs2Y",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqMVRBDLYs2Y",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLYcPRcLYs2Z",
        "colab_type": "text"
      },
      "source": [
        "## 1. Check for Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HJa-tQ20Ys2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('No of duplicates in train: {}'.format(sum(train.duplicated())))\n",
        "print('No of duplicates in test : {}'.format(sum(test.duplicated())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks6v9U76Ys2b",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gc4o_IOYs2c",
        "colab_type": "text"
      },
      "source": [
        "## 2. Checking for NaN/null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEWh5m90Ys2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('We have {} NaN/Null values in train'.format(train.isnull().values.sum()))\n",
        "print('We have {} NaN/Null values in test'.format(test.isnull().values.sum()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF9IcYSvYs2g",
        "colab_type": "text"
      },
      "source": [
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVLb2WG9Ys2g",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCxelPgXYs2h",
        "colab_type": "text"
      },
      "source": [
        "## 3. Check for data imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwq3Dxo1Ys2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['font.family'] = 'Dejavu Sans'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXptNMkdYs2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "plt.title('Data provided by each user', fontsize=20)\n",
        "sns.countplot(x='subject',hue='ActivityName', data = train)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgQlK9AZYs2k",
        "colab_type": "text"
      },
      "source": [
        "> We have got almost same number of reading from all the subjects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "HpJbAjkEYs2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title('No of Datapoints per Activity', fontsize=15)\n",
        "sns.countplot(train.ActivityName)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N04s6NbSYs2m",
        "colab_type": "text"
      },
      "source": [
        "### Observation\n",
        "> Our data is well balanced (almost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icN-PEjcYs2n",
        "colab_type": "text"
      },
      "source": [
        "## 4. Changing feature names "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_JO-BIHYs2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = train.columns\n",
        "\n",
        "# Removing '()' from column names\n",
        "columns = columns.str.replace('[()]','')\n",
        "columns = columns.str.replace('[-]', '')\n",
        "columns = columns.str.replace('[,]','')\n",
        "\n",
        "train.columns = columns\n",
        "test.columns = columns\n",
        "\n",
        "test.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwzvy_iDYs2p",
        "colab_type": "text"
      },
      "source": [
        "## 5. Save this dataframe in a csv files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_awo2Km3Ys2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv('/content/drive/My Drive/Motor/UCI_HAR_dataset/csv_files/train.csv', index=False)\n",
        "test.to_csv('/content/drive/My Drive/Motor/UCI_HAR_dataset/csv_files/test.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqPrDPPBYs2r",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxBRdcIuYs2r",
        "colab_type": "text"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2qJh1gVYs2r",
        "colab_type": "text"
      },
      "source": [
        "\"___Without domain knowledge, EDA has no meaning, without EDA a problem has no soul.___\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDRJf747Ys2s",
        "colab_type": "text"
      },
      "source": [
        "### 1. Featuring Engineering from Domain Knowledge \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnyRlQyzYs2s",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "+ __Static and Dynamic Activities__\n",
        "\n",
        "    - In static activities (sit, stand, lie down) motion information will not be very useful.\n",
        "\t- In the dynamic activities (Walking, WalkingUpstairs,WalkingDownstairs) motion info will be significant.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slar2OTJYs2s",
        "colab_type": "text"
      },
      "source": [
        "### 2. Stationary and Moving activities are completely different"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYo3kiUyYs2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set_palette(\"Set1\", desat=0.80)\n",
        "facetgrid = sns.FacetGrid(train, hue='ActivityName', height=6,aspect=2)\n",
        "facetgrid.map(sns.distplot,'tBodyAccMagmean', hist=False)\\\n",
        "    .add_legend()\n",
        "plt.annotate(\"Stationary Activities\", xy=(-0.956,17), xytext=(-0.9, 23), size=20,\\\n",
        "            va='center', ha='left',\\\n",
        "            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\n",
        "\n",
        "plt.annotate(\"Moving Activities\", xy=(0,3), xytext=(0.2, 9), size=20,\\\n",
        "            va='center', ha='left',\\\n",
        "            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7mZXiQiYs2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for plotting purposes taking datapoints of each activity to a different dataframe\n",
        "df1 = train[train['Activity']==1]\n",
        "df2 = train[train['Activity']==2]\n",
        "df3 = train[train['Activity']==3]\n",
        "df4 = train[train['Activity']==4]\n",
        "df5 = train[train['Activity']==5]\n",
        "df6 = train[train['Activity']==6]\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.subplot(2,2,1)\n",
        "plt.title('Stationary Activities(Zoomed in)')\n",
        "sns.distplot(df4['tBodyAccMagmean'],color = 'r',hist = False, label = 'Sitting')\n",
        "sns.distplot(df5['tBodyAccMagmean'],color = 'm',hist = False,label = 'Standing')\n",
        "sns.distplot(df6['tBodyAccMagmean'],color = 'c',hist = False, label = 'Laying')\n",
        "plt.axis([-1.01, -0.5, 0, 35])\n",
        "plt.legend(loc='center')\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.title('Moving Activities')\n",
        "sns.distplot(df1['tBodyAccMagmean'],color = 'red',hist = False, label = 'Walking')\n",
        "sns.distplot(df2['tBodyAccMagmean'],color = 'blue',hist = False,label = 'Walking Up')\n",
        "sns.distplot(df3['tBodyAccMagmean'],color = 'green',hist = False, label = 'Walking down')\n",
        "plt.legend(loc='center right')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or1OcKLzYs2w",
        "colab_type": "text"
      },
      "source": [
        "### 3. Magnitude of an acceleration can saperate it well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "zyKpnDS-Ys2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(7,7))\n",
        "sns.boxplot(x='ActivityName', y='tBodyAccMagmean',data=train, showfliers=False, saturation=1)\n",
        "plt.ylabel('Acceleration Magnitude mean')\n",
        "plt.axhline(y=-0.7, xmin=0.1, xmax=0.9,dashes=(5,5), c='g')\n",
        "plt.axhline(y=-0.05, xmin=0.4, dashes=(5,5), c='m')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMR-e6G1Ys2x",
        "colab_type": "text"
      },
      "source": [
        "__ Observations__:\n",
        "- If tAccMean is < -0.8 then the Activities are either Standing or Sitting or Laying.\n",
        "- If tAccMean is > -0.6 then the Activities are either Walking or WalkingDownstairs or WalkingUpstairs.\n",
        "- If tAccMean > 0.0 then the Activity is WalkingDownstairs.\n",
        "- We can classify 75% the Acitivity labels with some errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUaBCoWgYs2y",
        "colab_type": "text"
      },
      "source": [
        "### 4. Position of GravityAccelerationComponants also matters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU2U1bqfYs2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.boxplot(x='ActivityName', y='angleXgravityMean', data=train)\n",
        "plt.axhline(y=0.08, xmin=0.1, xmax=0.9,c='m',dashes=(5,3))\n",
        "plt.title('Angle between X-axis and Gravity_mean', fontsize=15)\n",
        "plt.xticks(rotation = 40)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jERhMUXeYs2z",
        "colab_type": "text"
      },
      "source": [
        "__ Observations__:\n",
        "* If angleX,gravityMean > 0 then Activity is Laying.\n",
        "* We can classify all datapoints belonging to Laying activity with just a single if else statement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWdnsszeYs20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.boxplot(x='ActivityName', y='angleYgravityMean', data = train, showfliers=False)\n",
        "plt.title('Angle between Y-axis and Gravity_mean', fontsize=15)\n",
        "plt.xticks(rotation = 40)\n",
        "plt.axhline(y=-0.22, xmin=0.1, xmax=0.8, dashes=(5,3), c='m')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRdYD91pYs22",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPMYymN6Ys22",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVnLKy2rYs22",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFWjp-6VYs23",
        "colab_type": "text"
      },
      "source": [
        "# Apply t-sne on the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Q2lpqnYs23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# performs t-sne with different perplexity values and their repective plots..\n",
        "def perform_tsne(X_data, y_data, perplexities, n_iter=1000, img_name_prefix='t-sne'):\n",
        "        \n",
        "    for index,perplexity in enumerate(perplexities):\n",
        "        # perform t-sne\n",
        "        print('\\nperforming tsne with perplexity {} and with {} iterations at max'.format(perplexity, n_iter))\n",
        "        X_reduced = TSNE(verbose=2, perplexity=perplexity).fit_transform(X_data)\n",
        "        print('Done..')\n",
        "        \n",
        "        # prepare the data for seaborn         \n",
        "        print('Creating plot for this t-sne visualization..')\n",
        "        df = pd.DataFrame({'x':X_reduced[:,0], 'y':X_reduced[:,1] ,'label':y_data})\n",
        "        \n",
        "        # draw the plot in appropriate place in the grid\n",
        "        sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,\\\n",
        "                   palette=\"Set1\",markers=['^','v','s','o', '1','2'])\n",
        "        plt.title(\"perplexity : {} and max_iter : {}\".format(perplexity, n_iter))\n",
        "        img_name = img_name_prefix + '_perp_{}_iter_{}.png'.format(perplexity, n_iter)\n",
        "        print('saving this plot as image in present working directory...')\n",
        "        plt.savefig(img_name)\n",
        "        plt.show()\n",
        "        print('Done')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uNutOkSiYs24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_pre_tsne = train.drop(['subject', 'Activity','ActivityName'], axis=1)\n",
        "y_pre_tsne = train['ActivityName']\n",
        "perform_tsne(X_data = X_pre_tsne,y_data=y_pre_tsne, perplexities =[2,5,10,20,50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6tf8IS1Ys26",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhQi-34LYs26",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLFZGpaLYs27",
        "colab_type": "text"
      },
      "source": [
        "## Apply Machine Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOfgVT71Ys27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/Motor/UCI_HAR_dataset/csv_files/train.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/Motor/UCI_HAR_dataset/csv_files/test.csv')\n",
        "print(train.shape, test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYdkm2lRYs28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xylm_njkYs2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get x_train and y_train from csv files\n",
        "x_train = train.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n",
        "y_train = train.ActivityName\n",
        "\n",
        "# get x_test and y_test from test csv file\n",
        "x_test = test.drop(['subject', 'Activity', 'ActivityName'], axis=1)\n",
        "y_test = test.ActivityName\n",
        "\n",
        "print('x_train and y_train : ({},{})'.format(x_train.shape, y_train.shape))\n",
        "print('x_test  and y_test  : ({},{})'.format(x_test.shape, y_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcog_HCwYs3A",
        "colab_type": "text"
      },
      "source": [
        "### Labels that are useful in plotting confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUq93UkuYs3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels=['LAYING', 'SITTING','STANDING','WALKING','WALKING_DOWNSTAIRS','WALKING_UPSTAIRS']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGdhkm_KYs3B",
        "colab_type": "text"
      },
      "source": [
        "### Function to plot the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTPM9UBfYs3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vkWaFeZYs3D",
        "colab_type": "text"
      },
      "source": [
        "### Generic function to run any model specified"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XF8oVf2Ys3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perform_model(model, X_train, y_train, X_test, y_test, class_labels, cm_normalize=True, \\\n",
        "                 print_cm=True, cm_cmap=plt.cm.Greens):    \n",
        "    # to store results at various phases\n",
        "    results = dict()\n",
        "    \n",
        "    # time at which model starts training \n",
        "    train_start_time = datetime.now()\n",
        "    print('training the model..')\n",
        "    model.fit(X_train, y_train)\n",
        "    print('Done \\n \\n')\n",
        "    train_end_time = datetime.now()\n",
        "    results['training_time'] =  train_end_time - train_start_time\n",
        "    print('training_time(HH:MM:SS.ms) - {}\\n\\n'.format(results['training_time']))\n",
        "    \n",
        "    \n",
        "    # predict test data\n",
        "    print('Predicting test data')\n",
        "    test_start_time = datetime.now()\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_end_time = datetime.now()\n",
        "    print('Done \\n \\n')\n",
        "    results['testing_time'] = test_end_time - test_start_time\n",
        "    print('testing time(HH:MM:SS:ms) - {}\\n\\n'.format(results['testing_time']))\n",
        "    results['predicted'] = y_pred\n",
        "   \n",
        "\n",
        "    # calculate overall accuracty of the model\n",
        "    accuracy = metrics.accuracy_score(y_true=y_test, y_pred=y_pred)\n",
        "    # store accuracy in results\n",
        "    results['accuracy'] = accuracy\n",
        "    print('---------------------')\n",
        "    print('|      Accuracy      |')\n",
        "    print('---------------------')\n",
        "    print('\\n    {}\\n\\n'.format(accuracy))\n",
        "    \n",
        "    \n",
        "    # confusion matrix\n",
        "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
        "    results['confusion_matrix'] = cm\n",
        "    if print_cm: \n",
        "        print('--------------------')\n",
        "        print('| Confusion Matrix |')\n",
        "        print('--------------------')\n",
        "        print('\\n {}'.format(cm))\n",
        "        \n",
        "    # plot confusin matrix\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.grid(b=False)\n",
        "    plot_confusion_matrix(cm, classes=class_labels, normalize=True, title='Normalized confusion matrix', cmap = cm_cmap)\n",
        "    plt.show()\n",
        "    \n",
        "    # get classification report\n",
        "    print('-------------------------')\n",
        "    print('| Classifiction Report |')\n",
        "    print('-------------------------')\n",
        "    classification_report = metrics.classification_report(y_test, y_pred)\n",
        "    # store report in results\n",
        "    results['classification_report'] = classification_report\n",
        "    print(classification_report)\n",
        "    \n",
        "    # add the trained  model to the results\n",
        "    results['model'] = model\n",
        "    \n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuskABK3Ys3G",
        "colab_type": "text"
      },
      "source": [
        "### Method to print the gridsearch Attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "708GardeYs3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_grid_search_attributes(model):\n",
        "    # Estimator that gave highest score among all the estimators formed in GridSearch\n",
        "    print('--------------------------')\n",
        "    print('|      Best Estimator     |')\n",
        "    print('--------------------------')\n",
        "    print('\\n\\t{}\\n'.format(model.best_estimator_))\n",
        "\n",
        "\n",
        "    # parameters that gave best results while performing grid search\n",
        "    print('--------------------------')\n",
        "    print('|     Best parameters     |')\n",
        "    print('--------------------------')\n",
        "    print('\\tParameters of best estimator : \\n\\n\\t{}\\n'.format(model.best_params_))\n",
        "\n",
        "\n",
        "    #  number of cross validation splits\n",
        "    print('---------------------------------')\n",
        "    print('|   No of CrossValidation sets   |')\n",
        "    print('--------------------------------')\n",
        "    print('\\n\\tTotal numbre of cross validation sets: {}\\n'.format(model.n_splits_))\n",
        "\n",
        "\n",
        "    # Average cross validated score of the best estimator, from the Grid Search \n",
        "    print('--------------------------')\n",
        "    print('|        Best Score       |')\n",
        "    print('--------------------------')\n",
        "    print('\\n\\tAverage Cross Validate scores of best estimator : \\n\\n\\t{}\\n'.format(model.best_score_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J44Ya54LYs3I",
        "colab_type": "text"
      },
      "source": [
        "# 1. Logistic Regression with Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKXwrtGiYs3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start Grid search\n",
        "parameters = {'C':[0.01, 0.1, 1, 10, 20, 30], 'penalty':['l2','l1']}\n",
        "log_reg = linear_model.LogisticRegression()\n",
        "log_reg_grid = GridSearchCV(log_reg, param_grid=parameters, cv=3, verbose=1, n_jobs=-1)\n",
        "log_reg_grid_results =  perform_model(log_reg_grid, x_train, y_train, x_test, y_test, class_labels=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VG1sGedYs3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.grid(b=False)\n",
        "plot_confusion_matrix(log_reg_grid_results['confusion_matrix'], classes=labels, cmap=plt.cm.Greens)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4pUE1inYs3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# observe the attributes of the model \n",
        "print_grid_search_attributes(log_reg_grid_results['model'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrToSLUHYs3N",
        "colab_type": "text"
      },
      "source": [
        "#  2. Linear SVC with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI582MXHYs3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'C':[0.125, 0.5, 1, 2, 8, 16]}\n",
        "lr_svc = LinearSVC(tol=0.00005)\n",
        "lr_svc_grid = GridSearchCV(lr_svc, param_grid=parameters, n_jobs=-1, verbose=1)\n",
        "lr_svc_grid_results = perform_model(lr_svc_grid, x_train, y_train, x_test, y_test, class_labels=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiYvr08cYs3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.grid(b=False)\n",
        "plot_confusion_matrix(lr_svc_grid_results['confusion_matrix'], classes=labels, cmap=plt.cm.Greens)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtXSdQjGYs3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_grid_search_attributes(lr_svc_grid_results['model'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fNW6KtOYs3U",
        "colab_type": "text"
      },
      "source": [
        "# 3.  RBF Kernel SVM with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePwTlT8AYs3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'C':[2,8,16],\\\n",
        "              'gamma': [ 0.0078125, 0.125, 2]}\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm_grid = GridSearchCV(rbf_svm,param_grid=parameters, n_jobs=-1)\n",
        "rbf_svm_grid_results = perform_model(rbf_svm_grid, x_train, y_train, x_test, y_test, class_labels=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx9vvVN6Ys3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.grid(b=False)\n",
        "plot_confusion_matrix(rbf_svm_grid_results['confusion_matrix'], classes=labels, cmap=plt.cm.Greens)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW0OIMlOYs3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_grid_search_attributes(rbf_svm_grid_results['model'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G_MDHzOYs3Z",
        "colab_type": "text"
      },
      "source": [
        "# 4. Decision Tree with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG8YZj0WYs3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'max_depth':np.arange(3,10,2)}\n",
        "dt = DecisionTreeClassifier()\n",
        "dt_grid = GridSearchCV(dt,param_grid=parameters, n_jobs=-1)\n",
        "dt_grid_results = perform_model(dt_grid, x_train, y_train, x_test, y_test, class_labels=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7dKBGzsYs3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.grid(b=False)\n",
        "plot_confusion_matrix(dt_grid_results['confusion_matrix'], classes=labels, cmap=plt.cm.Greens)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tmSHDWHYs3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_grid_search_attributes(dt_grid_results['model'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw0A1MMeYs3f",
        "colab_type": "text"
      },
      "source": [
        "# 5. Random Forest Classifier with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRLRLCfgYs3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {'n_estimators': np.arange(10,201,20), 'max_depth':np.arange(3,15,2)}\n",
        "rfc = RandomForestClassifier()\n",
        "rfc_grid = GridSearchCV(rfc, param_grid=params, n_jobs=-1)\n",
        "rfc_grid_results = perform_model(rfc_grid, x_train, y_train, x_test, y_test, class_labels=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBlSi0jeYs3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.grid(b=False)\n",
        "plot_confusion_matrix(rfc_grid_results['confusion_matrix'], classes=labels, cmap=plt.cm.Greens)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmhVM9fWYs3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_grid_search_attributes(rfc_grid_results['model'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhxQiJ_WYs3l",
        "colab_type": "text"
      },
      "source": [
        "# 6.  Gradient Boosted Decision Trees With GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLtlImvXYs3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = {'max_depth': np.arange(5,8,1), \\\n",
        "             'n_estimators':np.arange(130,170,10)}\n",
        "gbdt = GradientBoostingClassifier()\n",
        "gbdt_grid = GridSearchCV(gbdt, param_grid=param_grid, n_jobs=-1)\n",
        "gbdt_grid_results = perform_model(gbdt_grid, x_train, y_train, x_test, y_test, class_labels=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Az0V2RCYs3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.grid(b=False)\n",
        "plot_confusion_matrix(gbdt_grid_results['confusion_matrix'], classes=labels, cmap=plt.cm.Greens)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u75Gxu3GYs3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_grid_search_attributes(gbdt_grid_results['model'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRJaENsEYs3s",
        "colab_type": "text"
      },
      "source": [
        "# 7. Comparing all models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1hKjP2YYs3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('\\n                     Accuracy     Error')\n",
        "print('                     ----------   --------')\n",
        "print('Logistic Regression : {:.04}%       {:.04}%'.format(log_reg_grid_results['accuracy'] * 100,\\\n",
        "                                                  100-(log_reg_grid_results['accuracy'] * 100)))\n",
        "\n",
        "print('Linear SVC          : {:.04}%       {:.04}% '.format(lr_svc_grid_results['accuracy'] * 100,\\\n",
        "                                                        100-(lr_svc_grid_results['accuracy'] * 100)))\n",
        "\n",
        "print('RBF SVM classifier  : {:.04}%      {:.04}% '.format(rbf_svm_grid_results['accuracy'] * 100,\\\n",
        "                                                          100-(rbf_svm_grid_results['accuracy'] * 100)))\n",
        "\n",
        "print('DecisionTree        : {:.04}%      {:.04}% '.format(dt_grid_results['accuracy'] * 100,\\\n",
        "                                                        100-(dt_grid_results['accuracy'] * 100)))\n",
        "\n",
        "print('Random Forest       : {:.04}%      {:.04}% '.format(rfc_grid_results['accuracy'] * 100,\\\n",
        "                                                           100-(rfc_grid_results['accuracy'] * 100)))\n",
        "print('GradientBoosting DT : {:.04}%      {:.04}% '.format(rfc_grid_results['accuracy'] * 100,\\\n",
        "                                                        100-(rfc_grid_results['accuracy'] * 100)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxpoxeRfYs3u",
        "colab_type": "text"
      },
      "source": [
        "> We can choose ___Logistic regression___ or ___Linear SVC___ or ___RBF SVM___."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEMbO89BYs3u",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB6xfeKTYs3u",
        "colab_type": "text"
      },
      "source": [
        "## Apply Deep Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGDLEQMbYs3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Activities are the class labels\n",
        "# It is a 6 class classification\n",
        "ACTIVITIES = {\n",
        "    0: 'WALKING',\n",
        "    1: 'WALKING_UPSTAIRS',\n",
        "    2: 'WALKING_DOWNSTAIRS',\n",
        "    3: 'SITTING',\n",
        "    4: 'STANDING',\n",
        "    5: 'LAYING',\n",
        "}\n",
        "\n",
        "# Data directory\n",
        "DATADIR = 'UCI_HAR_Dataset'\n",
        "\n",
        "# Raw data signals\n",
        "# Signals are from Accelerometer and Gyroscope\n",
        "# The signals are in x,y,z directions\n",
        "# Sensor signals are filtered to have only body acceleration\n",
        "# excluding the acceleration due to gravity\n",
        "# Triaxial acceleration from the accelerometer is total acceleration\n",
        "SIGNALS = [\n",
        "    \"body_acc_x\",\n",
        "    \"body_acc_y\",\n",
        "    \"body_acc_z\",\n",
        "    \"body_gyro_x\",\n",
        "    \"body_gyro_y\",\n",
        "    \"body_gyro_z\",\n",
        "    \"total_acc_x\",\n",
        "    \"total_acc_y\",\n",
        "    \"total_acc_z\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnDxL0fBYs3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function to print the confusion matrix\n",
        "def confusion_matrix_dl(Y_true, Y_pred):\n",
        "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
        "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
        "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])\n",
        "\n",
        "# Utility function to read the data from csv file\n",
        "def _read_csv(filename):\n",
        "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
        "\n",
        "# Utility function to load the load\n",
        "def load_signals(subset):\n",
        "    signals_data = []\n",
        "\n",
        "    for signal in SIGNALS:\n",
        "        filename = f'/content/drive/My Drive/Motor/UCI_HAR_dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
        "        signals_data.append(\n",
        "            _read_csv(filename).as_matrix()\n",
        "        ) \n",
        "\n",
        "    # Transpose is used to change the dimensionality of the output,\n",
        "    # aggregating the signals by combination of sample/timestep.\n",
        "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
        "    return np.transpose(signals_data, (1, 2, 0))\n",
        "\n",
        "def load_y(subset):\n",
        "    \"\"\"\n",
        "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
        "    that represents a human activity. We return a binary representation of \n",
        "    every sample objective as a 6 bits vector using One Hot Encoding\n",
        "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
        "    \"\"\"\n",
        "    filename = f'/content/drive/My Drive/Motor/UCI_HAR_dataset/{subset}/y_{subset}.txt'\n",
        "    y = _read_csv(filename)[0]\n",
        "\n",
        "    return pd.get_dummies(y).as_matrix()\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"\n",
        "    Obtain the dataset from multiple files.\n",
        "    Returns: X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    X_train, X_test = load_signals('train'), load_signals('test')\n",
        "    y_train, y_test = load_y('train'), load_y('test')\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Utility function to count the number of classes\n",
        "def _count_classes(y):\n",
        "    return len(set([tuple(category) for category in y]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnsT27poYs3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configuring a session\n",
        "session_conf = tf.ConfigProto(\n",
        "    intra_op_parallelism_threads=1,\n",
        "    inter_op_parallelism_threads=1\n",
        ")\n",
        "\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC9RA3ccYs3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initializing parameters\n",
        "epochs = 30\n",
        "batch_size = 16\n",
        "n_hidden = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPILFP7iYs31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the train and test data\n",
        "x_train, x_test, y_train, y_test = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyLb5YsyYs34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timesteps = len(x_train[0])\n",
        "input_dim = len(x_train[0][0])\n",
        "n_classes = _count_classes(y_train)\n",
        "\n",
        "print(timesteps)\n",
        "print(input_dim)\n",
        "print(len(x_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhTZPDdIYs35",
        "colab_type": "text"
      },
      "source": [
        "## 1-Layer of LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdFjknkUYs35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initiliazing the sequential model\n",
        "model = Sequential()\n",
        "# Configuring the parameters\n",
        "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
        "# Adding a dropout layer\n",
        "model.add(Dropout(0.5))\n",
        "# Adding a dense output layer with sigmoid activation\n",
        "model.add(Dense(n_classes, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFuST1AEYs36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4QnYTPfYs38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training the model\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          batch_size=batch_size,\n",
        "          validation_data=(x_test, y_test),\n",
        "          epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofCZqNCeYs38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confusion Matrix\n",
        "print(confusion_matrix_dl(y_test, model.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ3rjuhqYs39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdFquB5dYs3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGuIkrXeYs3_",
        "colab_type": "text"
      },
      "source": [
        "## 2-Layer of LSTM with more hyperparameter tunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9NeXlOGYs4A",
        "colab_type": "text"
      },
      "source": [
        "### Tuning the Number of neurons with dropout of 0.50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNBSK0jpYs4A",
        "colab_type": "text"
      },
      "source": [
        "Configuration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNVpHq_sYs4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initializing parameters\n",
        "n_epochs = 30\n",
        "n_batch = 16\n",
        "n_classes = _count_classes(y_train)\n",
        "\n",
        "# Bias regularizer value - we will use elasticnet\n",
        "reg = L1L2(0.01, 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmRF7fFIYs4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Confusion Matrix\n",
        "def plot_confusion_matrix_lstm(y_test, y_predict):\n",
        "    result = confusion_matrix(y_test, y_predict)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(result, \n",
        "                xticklabels= list(ACTIVITIES.values()), \n",
        "                yticklabels=list(ACTIVITIES.values()), \n",
        "                annot=True, fmt=\"d\");\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()  \n",
        "    \n",
        "# Plot train and cross validation loss\n",
        "def plot_train_cv_loss(trained_model, epochs, colors=['b']):\n",
        "    fig, ax = plt.subplots(1,1)    \n",
        "    ax.set_xlabel('epoch') \n",
        "    ax.set_ylabel('Categorical Crossentropy Loss')\n",
        "    x_axis_values = list(range(1,epochs+1))\n",
        "\n",
        "    validation_loss = trained_model.history['val_loss']\n",
        "    train_loss = trained_model.history['loss']   \n",
        "    \n",
        "    ax.plot(x_axis_values, validation_loss, 'b', label=\"Validation Loss\")\n",
        "    ax.plot(x_axis_values, train_loss, 'r', label=\"Train Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    fig.canvas.draw() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEpJMmJ4Ys4F",
        "colab_type": "text"
      },
      "source": [
        "### 32 neurons in LSTM Layer with dropout of 0.50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "QrljKitIYs4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model execution\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(timesteps, input_dim), return_sequences=True,bias_regularizer=reg ))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.50))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.50))\n",
        "model.add(Dense(n_classes, activation='sigmoid'))\n",
        "print(\"Model Summary: \")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv5vaqkEYs4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "          optimizer='adam',\n",
        "          metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "---1vK9kYs4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = datetime.now()\n",
        "\n",
        "# Training the model\n",
        "trained_model  = model.fit(x_train,\n",
        "                           y_train,\n",
        "                           batch_size=n_batch,\n",
        "                           validation_data=(x_test, y_test),\n",
        "                           epochs=n_epochs)\n",
        "\n",
        "print(\"\\n Time Taken: \",datetime.now() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzKVPxQmYs4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()\n",
        "print()\n",
        "\n",
        "# Plot train and cross validation error\n",
        "plot_train_cv_loss(trained_model, n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXCPJmkpYs4L",
        "colab_type": "text"
      },
      "source": [
        "From epoch 10, we starts to overfit the model, so best value for epoch is 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGkdXB1kYs4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy: %f%%\" % (scores[1]*100))\n",
        "print()\n",
        "\n",
        "Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(y_test, axis=1)])\n",
        "Y_predictions = pd.Series([ACTIVITIES[y] for y in np.argmax(model.predict(x_test), axis=1)])\n",
        "\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix_lstm(Y_true, Y_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUtJmBTOYs4M",
        "colab_type": "text"
      },
      "source": [
        "<br/>\n",
        "<br/>\n",
        "\n",
        "### 48 neurons in LSTM Layer with dropout of 0.50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWhK0_jFYs4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model execution\n",
        "model = Sequential()\n",
        "model.add(LSTM(48, input_shape=(timesteps, input_dim), return_sequences=True, bias_regularizer=reg))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.50))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.50))\n",
        "model.add(Dense(n_classes, activation='sigmoid'))\n",
        "print(\"Model Summary: \")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnfcOsaDYs4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "          optimizer='adam',\n",
        "          metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpPBwj80Ys4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = datetime.now()\n",
        "\n",
        "# Training the model\n",
        "trained_model  = model.fit(x_train,\n",
        "                           y_train,\n",
        "                           batch_size=n_batch,\n",
        "                           validation_data=(x_test, y_test),\n",
        "                           epochs=n_epochs)\n",
        "\n",
        "print(\"\\n Time Taken: \",datetime.now() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aan54BEiYs4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()\n",
        "print()\n",
        "\n",
        "# Plot train and cross validation error\n",
        "plot_train_cv_loss(trained_model, n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGWcGXX0Ys4S",
        "colab_type": "text"
      },
      "source": [
        "From epoch 7, we starts to overfit the model, so best value for epoch is 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzSFdRi9Ys4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy: %f%%\" % (scores[1]*100))\n",
        "print()\n",
        "\n",
        "Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(y_test, axis=1)])\n",
        "Y_predictions = pd.Series([ACTIVITIES[y] for y in np.argmax(model.predict(x_test), axis=1)])\n",
        "\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix_lstm(Y_true, Y_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U5VNpCnYs4T",
        "colab_type": "text"
      },
      "source": [
        "<br/>\n",
        "<br/>\n",
        "\n",
        "### 64 neurons in LSTM Layer with dropout of 0.50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiWZeeBUYs4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model execution\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(timesteps, input_dim), return_sequences=True, bias_regularizer=reg))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.50))\n",
        "model.add(LSTM(48))\n",
        "model.add(Dropout(0.50))\n",
        "model.add(Dense(n_classes, activation='sigmoid'))\n",
        "print(\"Model Summary: \")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hx72fYAYs4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "          optimizer='adam',\n",
        "          metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5vfAVcTYs4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = datetime.now()\n",
        "\n",
        "# Training the model\n",
        "trained_model  = model.fit(x_train,\n",
        "                           y_train,\n",
        "                           batch_size=n_batch,\n",
        "                           validation_data=(x_test, y_test),\n",
        "                           epochs=n_epochs)\n",
        "\n",
        "print(\"\\n Time Taken: \",datetime.now() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3YgosbQYs4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()\n",
        "print()\n",
        "\n",
        "# Plot train and cross validation error\n",
        "plot_train_cv_loss(trained_model, n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui6OWNEuYs4Y",
        "colab_type": "text"
      },
      "source": [
        "From epoch 8, we starts to overfit the model, so best value for epoch is 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjjnN9KqYs4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy: %f%%\" % (scores[1]*100))\n",
        "print()\n",
        "\n",
        "Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(y_test, axis=1)])\n",
        "Y_predictions = pd.Series([ACTIVITIES[y] for y in np.argmax(model.predict(x_test), axis=1)])\n",
        "\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix_lstm(Y_true, Y_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7a6WohIYs4Z",
        "colab_type": "text"
      },
      "source": [
        "<br/>\n",
        "<br/>\n",
        "\n",
        "### Tuning the number of neurons with dropout of 0.70"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTmM8YUBYs4Z",
        "colab_type": "text"
      },
      "source": [
        "Configuration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym05GDIGYs4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initializing parameters\n",
        "n_epochs = 30\n",
        "n_batch = 16\n",
        "n_classes = _count_classes(y_train)\n",
        "\n",
        "# Bias regularizer value - we will use elasticnet\n",
        "reg = L1L2(0.01, 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8x-RiGGYs4a",
        "colab_type": "text"
      },
      "source": [
        "### 32 neurons in LSTM Layer with dropout of 0.70"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1b7VT8xYs4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model execution\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(timesteps, input_dim), return_sequences=True, bias_regularizer=reg))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.70))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.70))\n",
        "model.add(Dense(n_classes, activation='sigmoid'))\n",
        "print(\"Model Summary: \")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WecQNzUOYs4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "          optimizer='adam',\n",
        "          metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en0i8pDlYs4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = datetime.now()\n",
        "\n",
        "# Training the model\n",
        "trained_model  = model.fit(x_train,\n",
        "                           y_train,\n",
        "                           batch_size=n_batch,\n",
        "                           validation_data=(x_test, y_test),\n",
        "                           epochs=n_epochs)\n",
        "\n",
        "print(\"\\n Time Taken: \",datetime.now() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NLHWVnZYs4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()\n",
        "print()\n",
        "\n",
        "# Plot train and cross validation error\n",
        "plot_train_cv_loss(trained_model, n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SINfMKuzYs4g",
        "colab_type": "text"
      },
      "source": [
        "From epoch 9, we starts to overfit the model, so best value for epoch is 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GeZUV9_Ys4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy: %f%%\" % (scores[1]*100))\n",
        "print()\n",
        "\n",
        "Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(y_test, axis=1)])\n",
        "Y_predictions = pd.Series([ACTIVITIES[y] for y in np.argmax(model.predict(x_test), axis=1)])\n",
        "\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix_lstm(Y_true, Y_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2zyXLTOYs4h",
        "colab_type": "text"
      },
      "source": [
        "<br/>\n",
        "<br/>\n",
        "\n",
        "### 48 neurons in LSTM Layer with dropout of 0.70"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__Ik8qRdYs4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model execution\n",
        "model = Sequential()\n",
        "model.add(LSTM(48, input_shape=(timesteps, input_dim), return_sequences=True, bias_regularizer=reg))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.70))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.70))\n",
        "model.add(Dense(n_classes, activation='sigmoid'))\n",
        "print(\"Model Summary: \")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkwKGRBCYs4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "          optimizer='adam',\n",
        "          metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2012BYWYs4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = datetime.now()\n",
        "\n",
        "# Training the model\n",
        "trained_model  = model.fit(x_train,\n",
        "                           y_train,\n",
        "                           batch_size=n_batch,\n",
        "                           validation_data=(x_test, y_test),\n",
        "                           epochs=n_epochs)\n",
        "\n",
        "print(\"\\n Time Taken: \",datetime.now() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyXmlwDaYs4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()\n",
        "print()\n",
        "\n",
        "# Plot train and cross validation error\n",
        "plot_train_cv_loss(trained_model, n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feATjgUoYs4m",
        "colab_type": "text"
      },
      "source": [
        "From epoch 16, we starts to overfit the model, so best value for epoch is 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpoZ6yVoYs4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy: %f%%\" % (scores[1]*100))\n",
        "print()\n",
        "\n",
        "Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(y_test, axis=1)])\n",
        "Y_predictions = pd.Series([ACTIVITIES[y] for y in np.argmax(model.predict(x_test), axis=1)])\n",
        "\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix_lstm(Y_true, Y_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUkEeTJ-Ys4o",
        "colab_type": "text"
      },
      "source": [
        "<br/>\n",
        "<br/>\n",
        "\n",
        "### 64 neurons in LSTM Layer with dropout of 0.70"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3woY-tRYs4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model execution\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(timesteps, input_dim), return_sequences=True, bias_regularizer=reg))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.70))\n",
        "model.add(LSTM(48))\n",
        "model.add(Dropout(0.70))\n",
        "model.add(Dense(n_classes, activation='sigmoid'))\n",
        "print(\"Model Summary: \")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uNaMyejYs4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "          optimizer='adam',\n",
        "          metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixOv5Qw1Ys4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = datetime.now()\n",
        "\n",
        "# Training the model\n",
        "trained_model  = model.fit(x_train,\n",
        "                           y_train,\n",
        "                           batch_size=n_batch,\n",
        "                           validation_data=(x_test, y_test),\n",
        "                           epochs=n_epochs)\n",
        "\n",
        "print(\"\\n Time Taken: \",datetime.now() - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKwbCvaPYs4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()\n",
        "print()\n",
        "\n",
        "# Plot train and cross validation error\n",
        "plot_train_cv_loss(trained_model, n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFo7DqomYs4r",
        "colab_type": "text"
      },
      "source": [
        "From epoch 7, we starts to overfit the model, so best value for epoch is 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e0gNSrbYs4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy: %f%%\" % (scores[1]*100))\n",
        "print()\n",
        "\n",
        "Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(y_test, axis=1)])\n",
        "Y_predictions = pd.Series([ACTIVITIES[y] for y in np.argmax(model.predict(x_test), axis=1)])\n",
        "\n",
        "# Confusion Matrix\n",
        "plot_confusion_matrix_lstm(Y_true, Y_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C1bQPtCYs4s",
        "colab_type": "text"
      },
      "source": [
        "<h2> Conclusion </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P51m5kScYs4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "ptable = PrettyTable()\n",
        "ptable.title = \" Model Comparision \"\n",
        "ptable.field_names = [\"LSTM Layers\",'No. of Neurons in LSTM Layer','Dropout', 'Best Epoch']\n",
        "ptable.add_row([\"2\",\"32\",\"0.50\",\"10\"])\n",
        "ptable.add_row([\"2\",\"48\",\"0.50\",\"7\"])\n",
        "ptable.add_row([\"2\",\"64\",\"0.50\",\"8\"])\n",
        "ptable.add_row([\"\\n\",\"\\n\",\"\\n\",\"\\n\"])\n",
        "ptable.add_row([\"2\",\"32\",\"0.70\",\"9\"])\n",
        "ptable.add_row([\"2\",\"48\",\"0.70\",\"16\"])\n",
        "ptable.add_row([\"2\",\"64\",\"0.70\",\"7\"])\n",
        "print(ptable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoDs6SQoYs4t",
        "colab_type": "text"
      },
      "source": [
        "From all the plots, we have observed that 64 neurons with 0.50 dropout rate will be the good choice, among all the models."
      ]
    }
  ]
}